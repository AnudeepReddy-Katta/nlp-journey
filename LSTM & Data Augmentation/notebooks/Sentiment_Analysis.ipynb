{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnudeepReddy-Katta/SentimentAnalysis/blob/main/notebooks/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIX9YiwTyJhw",
        "outputId": "36f5982e-6ad0-41ea-a30f-b8a0e31cc015"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKMchc2csnLA"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('/content/drive/MyDrive/NLP/train_aug.csv', index_col=0)\n",
        "test = pd.read_csv('/content/drive/MyDrive/NLP/test.csv', index_col=0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "7pRycnZxtAs_",
        "outputId": "175ada2f-3c5e-47bf-fc03-2e73781aa192"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In gleefully , thumpingly hyperbolic terms , i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>These people would n't know subtle characteriz...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A grittily beautiful film that looks , sounds ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No more .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Although ... visually striking and slickly sta...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  sentiment_values\n",
              "0  In gleefully , thumpingly hyperbolic terms , i...                 2\n",
              "1  These people would n't know subtle characteriz...                 0\n",
              "2  A grittily beautiful film that looks , sounds ...                 3\n",
              "3                                          No more .                 1\n",
              "4  Although ... visually striking and slickly sta...                 1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "1_h1ZYPVtOcU",
        "outputId": "81c21ec3-89e6-4cc3-8564-b877481e9fcd"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You might be shocked to discover that Seinfeld...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>For a film that celebrates radical , nonconfor...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Films are made of little moments .</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The main problem being that it 's only a peek .</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Even the imaginative gore ca n't hide the must...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  sentiment_values\n",
              "0  You might be shocked to discover that Seinfeld...                 2\n",
              "1  For a film that celebrates radical , nonconfor...                 2\n",
              "2                 Films are made of little moments .                 2\n",
              "3    The main problem being that it 's only a peek .                 2\n",
              "4  Even the imaginative gore ca n't hide the must...                 1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLvz4T3ctPmw",
        "outputId": "a9814ea9-4a81-45d1-be6c-a2c0d4df95d1"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1c53e53870>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fhJLUNwuH1H"
      },
      "source": [
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScIDzSIluNnV"
      },
      "source": [
        "fields = [('sentences', Sentence),('labels',Label)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFi8W2fDuWBl"
      },
      "source": [
        "example = [data.Example.fromlist([train.sentence[i],train.sentiment_values[i]], fields) for i in range(train.shape[0])] "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQmbhzFOuikt"
      },
      "source": [
        "train_data = data.Dataset(example, fields)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmWpxicCu20T"
      },
      "source": [
        "example_test = [data.Example.fromlist([test.sentence[i],test.sentiment_values[i]], fields) for i in range(test.shape[0])] "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CPNf-Jju-S6"
      },
      "source": [
        "valid_data = data.Dataset(example_test, fields)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9EgxUHuvjj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34246958-93fd-41ea-fbc4-734c6b244939"
      },
      "source": [
        "(len(train_data), len(valid_data))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12483, 2371)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv1umDYdwtDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7595996-b3e3-48b2-8cd6-b2efb26aa31c"
      },
      "source": [
        "vars(valid_data.examples[50])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 3,\n",
              " 'sentences': ['Watching',\n",
              "  'Austin',\n",
              "  'Powers',\n",
              "  'in',\n",
              "  'Goldmember',\n",
              "  'is',\n",
              "  'like',\n",
              "  'binging',\n",
              "  'on',\n",
              "  'cotton',\n",
              "  'candy',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaL1-zEeww3o"
      },
      "source": [
        "Sentence.build_vocab(train_data)\n",
        "Label.build_vocab(train_data)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhYbqlxMw2TZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470c1bb4-00b1-4e32-8832-3483037872c3"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  18141\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 11147), (',', 9917), ('the', 8363), ('and', 6119), ('of', 6047), ('a', 5985), ('to', 4226), ('-', 3796), (\"'s\", 3497), ('is', 3463)]\n",
            "Labels :  defaultdict(None, {1: 0, 0: 1, 2: 2, 3: 3, 4: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLScwnPyw8Ua"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNJ0uvtIw_3L"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data), batch_size = 64, \n",
        "                                                            sort_key = lambda x: len(x.sentences),\n",
        "                                                            sort_within_batch=True, \n",
        "                                                            device = device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWEC9KbBxHcc"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Sentence.vocab.stoi, tokens)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GkL84wLxP1z"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                           bidirectional=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0i6IJmDxQny"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Sentence.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 5\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxFkfZZAxV16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646ea7fe-0edc-4a3f-b155-2a6d511062a9"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(18141, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            ")\n",
            "The model has 6,006,005 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcrSS6VMxZWT"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynfcEJh0xbdc"
      },
      "source": [
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        sent, sent_lengths = batch.sentences   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(sent, sent_lengths).squeeze()  \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = categorical_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqDe8IHDxbj0"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            sent, sent_lengths = batch.sentences\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(sent, sent_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = categorical_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb2aZS4exbqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a39a79-f6e4-4559-e83c-b8223aece919"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.588 | Train Acc: 26.64%\n",
            "\t Val. Loss: 1.567 |  Val. Acc: 30.89% \n",
            "\n",
            "\tTrain Loss: 1.473 | Train Acc: 42.43%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 31.22% \n",
            "\n",
            "\tTrain Loss: 1.335 | Train Acc: 57.54%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 32.46% \n",
            "\n",
            "\tTrain Loss: 1.216 | Train Acc: 70.45%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 35.18% \n",
            "\n",
            "\tTrain Loss: 1.132 | Train Acc: 78.43%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 33.72% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoxVyOCb0anR"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_sentence(sentence):\n",
        "    \n",
        "    categories = {0: \"very negative\", 1:\"negative\", 2:\"neutral\", 3:\"positive\", 4:\"very positive\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    pred = prediction.argmax(1)\n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BqJ_rYDB8pOP",
        "outputId": "fbd8a60d-34cc-42cd-d9ad-f1f47cb6f6f8"
      },
      "source": [
        "classify_sentence(\"The notion that bombing buildings is the funniest thing in the world goes entirely unexamined in this startlingly unfunny comedy .\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SciXp7FK9wXG",
        "outputId": "ffd25a64-e092-41ff-cde4-60b022e28292"
      },
      "source": [
        "classify_sentence(\"Visually rather stunning , but ultimately a handsome-looking bore , the true creativity would have been to hide Treasure Planet entirely and completely reimagine it .\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'very negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t21BMn3t92Ji",
        "outputId": "a727aa3f-09b2-4981-9a6b-8f228465a616"
      },
      "source": [
        "\n",
        "classify_sentence(\"Miller is playing so free with emotions , and the fact that children are hostages to fortune , that he makes the audience hostage to his swaggering affectation of seriousness .\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'positive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a5cyJStM95ZC",
        "outputId": "9140755c-ccf2-44ea-b178-76b7de7bc351"
      },
      "source": [
        "classify_sentence(\"Schaeffer has to find some hook on which to hang his persistently useless movies , and it might as well be the resuscitation of the middle-aged character .\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'very negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h48ecIXZ-G9Q",
        "outputId": "bb643686-5ff7-4c12-e308-7115c0ea814a"
      },
      "source": [
        "classify_sentence(\"There 's just no currency in deriding James Bond for being a clichÃ©d , doddering , misogynistic boy 's club .\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'neutral'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xHrm2jaw-N2U",
        "outputId": "74c2f87c-e78b-46ea-fba8-514270f57d43"
      },
      "source": [
        "classify_sentence(\"When the film ended , I felt tired and drained and wanted to lie on my own deathbed for a while .\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_3hq9rUF-T4B",
        "outputId": "3c90aed9-4322-4e09-967c-f02e850b2bd6"
      },
      "source": [
        "classify_sentence(\"Full of witless jokes , dealing in broad stereotypes and outrageously unbelievable scenarios , and saddled with a general air of misogyny\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B-3shT6F-YtD",
        "outputId": "d917a38b-462b-452a-ee44-c99cf4e40f76"
      },
      "source": [
        "classify_sentence(\"The film 's hackneyed message is not helped by the thin characterizations , nonexistent plot and pretentious visual style .\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qjSnAB5i-dVO",
        "outputId": "27cea786-9a8c-4424-9b9a-10ee17e01984"
      },
      "source": [
        "classify_sentence(\"Detox is ultimately a pointless endeavor .\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'very negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cbNJ5GfU-gXb",
        "outputId": "ad6e4703-4528-487d-abac-6b6662ecde08"
      },
      "source": [
        "classify_sentence(\"Too much of the humor falls flat .\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'very negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5vBz7fV4-ilC",
        "outputId": "d47169de-8ad3-4a5c-abd1-2c64894e6f6e"
      },
      "source": [
        "classify_sentence(\"No way I can believe this load of junk .\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}